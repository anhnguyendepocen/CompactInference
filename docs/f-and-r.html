<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 F and R | A Compact Guide to Classical Inference</title>
  <meta name="description" content="Chapter 8 F and R | A Compact Guide to Classical Inference" />
  <meta name="generator" content="bookdown 0.14 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 F and R | A Compact Guide to Classical Inference" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 F and R | A Compact Guide to Classical Inference" />
  
  
  

<meta name="author" content="Daniel Kaplan" />


<meta name="date" content="2019-12-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="effect-size.html"/>
<link rel="next" href="confidence-intervals.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>



<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Compact Guide to Classical Inference</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="what-is-classical-inference.html"><a href="what-is-classical-inference.html"><i class="fa fa-check"></i><b>1</b> What is classical inference?</a><ul>
<li class="chapter" data-level="" data-path="what-is-classical-inference.html"><a href="what-is-classical-inference.html#and-why-should-i-read-this-book"><i class="fa fa-check"></i>… and why should I read this book?</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data-and-variables.html"><a href="data-and-variables.html"><i class="fa fa-check"></i><b>2</b> Data and variables</a><ul>
<li class="chapter" data-level="2.1" data-path="data-and-variables.html"><a href="data-and-variables.html#data-frames"><i class="fa fa-check"></i><b>2.1</b> Data frames</a></li>
<li class="chapter" data-level="2.2" data-path="data-and-variables.html"><a href="data-and-variables.html#tabulations"><i class="fa fa-check"></i><b>2.2</b> Tabulations</a></li>
<li class="chapter" data-level="2.3" data-path="data-and-variables.html"><a href="data-and-variables.html#quantitative-and-categorical-variables"><i class="fa fa-check"></i><b>2.3</b> Quantitative and categorical variables</a></li>
<li class="chapter" data-level="2.4" data-path="data-and-variables.html"><a href="data-and-variables.html#response-and-explanatory-variables"><i class="fa fa-check"></i><b>2.4</b> Response and explanatory variables</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="measuring-variation.html"><a href="measuring-variation.html"><i class="fa fa-check"></i><b>3</b> Measuring variation</a><ul>
<li class="chapter" data-level="3.1" data-path="measuring-variation.html"><a href="measuring-variation.html#variance-of-a-numerical-variable"><i class="fa fa-check"></i><b>3.1</b> Variance of a numerical variable</a></li>
<li class="chapter" data-level="3.2" data-path="measuring-variation.html"><a href="measuring-variation.html#variance-of-a-categorical-variable"><i class="fa fa-check"></i><b>3.2</b> Variance of a categorical variable?</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="modeling-variation.html"><a href="modeling-variation.html"><i class="fa fa-check"></i><b>4</b> Modeling variation</a><ul>
<li class="chapter" data-level="4.1" data-path="modeling-variation.html"><a href="modeling-variation.html#statistical-models"><i class="fa fa-check"></i><b>4.1</b> Statistical models</a></li>
<li class="chapter" data-level="4.2" data-path="modeling-variation.html"><a href="modeling-variation.html#quantitative-response-variables"><i class="fa fa-check"></i><b>4.2</b> Quantitative response variables</a></li>
<li class="chapter" data-level="4.3" data-path="modeling-variation.html"><a href="modeling-variation.html#proportions-and-indicator-variables"><i class="fa fa-check"></i><b>4.3</b> Proportions and indicator variables</a></li>
<li class="chapter" data-level="4.4" data-path="modeling-variation.html"><a href="modeling-variation.html#a-taxonomy-of-simple-models"><i class="fa fa-check"></i><b>4.4</b> A taxonomy of simple models</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="model-values.html"><a href="model-values.html"><i class="fa fa-check"></i><b>5</b> Model values</a><ul>
<li class="chapter" data-level="5.1" data-path="model-values.html"><a href="model-values.html#model-fitting-a-contest-between-candidate-models"><i class="fa fa-check"></i><b>5.1</b> Model fitting: A contest between candidate models</a></li>
<li class="chapter" data-level="5.2" data-path="model-values.html"><a href="model-values.html#variance-of-model-values"><i class="fa fa-check"></i><b>5.2</b> Variance of model values</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="degrees-of-flexibility.html"><a href="degrees-of-flexibility.html"><i class="fa fa-check"></i><b>6</b> Degrees of flexibility</a><ul>
<li class="chapter" data-level="6.1" data-path="degrees-of-flexibility.html"><a href="degrees-of-flexibility.html#one-degree-of-flexibility"><i class="fa fa-check"></i><b>6.1</b> One degree of flexibility</a></li>
<li class="chapter" data-level="6.2" data-path="degrees-of-flexibility.html"><a href="degrees-of-flexibility.html#multiple-degrees-of-flexibility"><i class="fa fa-check"></i><b>6.2</b> Multiple degrees of flexibility</a></li>
<li class="chapter" data-level="6.3" data-path="degrees-of-flexibility.html"><a href="degrees-of-flexibility.html#covariates"><i class="fa fa-check"></i><b>6.3</b> Covariates</a></li>
<li class="chapter" data-level="6.4" data-path="degrees-of-flexibility.html"><a href="degrees-of-flexibility.html#flexibility-literally"><i class="fa fa-check"></i><b>6.4</b> Flexibility, literally</a></li>
<li class="chapter" data-level="6.5" data-path="degrees-of-flexibility.html"><a href="degrees-of-flexibility.html#degrees-of-flexibility-and-freedom"><i class="fa fa-check"></i><b>6.5</b> Degrees of flexibility and freedom</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="effect-size.html"><a href="effect-size.html"><i class="fa fa-check"></i><b>7</b> Effect size</a><ul>
<li class="chapter" data-level="7.1" data-path="effect-size.html"><a href="effect-size.html#slopes-and-differences"><i class="fa fa-check"></i><b>7.1</b> Slopes and differences</a></li>
<li class="chapter" data-level="7.2" data-path="effect-size.html"><a href="effect-size.html#risk"><i class="fa fa-check"></i><b>7.2</b> Risk</a></li>
<li class="chapter" data-level="7.3" data-path="effect-size.html"><a href="effect-size.html#simple-changes-in-input"><i class="fa fa-check"></i><b>7.3</b> Simple changes in input</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="f-and-r.html"><a href="f-and-r.html"><i class="fa fa-check"></i><b>8</b> F and R</a><ul>
<li class="chapter" data-level="8.1" data-path="f-and-r.html"><a href="f-and-r.html#the-f-statistic"><i class="fa fa-check"></i><b>8.1</b> The F statistic</a></li>
<li class="chapter" data-level="8.2" data-path="f-and-r.html"><a href="f-and-r.html#whats-the-meaning-of-f"><i class="fa fa-check"></i><b>8.2</b> What’s the meaning of F?</a></li>
<li class="chapter" data-level="8.3" data-path="f-and-r.html"><a href="f-and-r.html#r-squared"><i class="fa fa-check"></i><b>8.3</b> R-squared</a></li>
<li class="chapter" data-level="8.4" data-path="f-and-r.html"><a href="f-and-r.html#f-in-statistics-books"><i class="fa fa-check"></i><b>8.4</b> F in statistics books</a></li>
<li class="chapter" data-level="8.5" data-path="f-and-r.html"><a href="f-and-r.html#explaining-the-form-of-f"><i class="fa fa-check"></i><b>8.5</b> Explaining the form of F</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="confidence-intervals.html"><a href="confidence-intervals.html"><i class="fa fa-check"></i><b>9</b> Confidence intervals</a><ul>
<li class="chapter" data-level="9.1" data-path="confidence-intervals.html"><a href="confidence-intervals.html#confidence-intervals-from-f"><i class="fa fa-check"></i><b>9.1</b> Confidence intervals from F</a></li>
<li class="chapter" data-level="9.2" data-path="confidence-intervals.html"><a href="confidence-intervals.html#why-4"><i class="fa fa-check"></i><b>9.2</b> Why 4?</a></li>
<li class="chapter" data-level="9.3" data-path="confidence-intervals.html"><a href="confidence-intervals.html#situations-where-f-doesnt-tell-enough"><i class="fa fa-check"></i><b>9.3</b> Situations where F doesn’t tell enough</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="so-called-statistical-significance.html"><a href="so-called-statistical-significance.html"><i class="fa fa-check"></i><b>10</b> So-called “statistical significance”</a><ul>
<li class="chapter" data-level="10.1" data-path="so-called-statistical-significance.html"><a href="so-called-statistical-significance.html#calculating-a-p-value"><i class="fa fa-check"></i><b>10.1</b> Calculating a p-value</a></li>
<li class="chapter" data-level="10.2" data-path="so-called-statistical-significance.html"><a href="so-called-statistical-significance.html#history-and-criticism"><i class="fa fa-check"></i><b>10.2</b> History and criticism</a></li>
<li class="chapter" data-level="10.3" data-path="so-called-statistical-significance.html"><a href="so-called-statistical-significance.html#appendix-when-df-geq-2"><i class="fa fa-check"></i><b>10.3</b> Appendix: When <span class="math inline">\(df \geq 2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="simple-means-and-proportions.html"><a href="simple-means-and-proportions.html"><i class="fa fa-check"></i><b>11</b> Simple means and proportions</a><ul>
<li class="chapter" data-level="11.1" data-path="simple-means-and-proportions.html"><a href="simple-means-and-proportions.html#no-flexibility-df-0"><i class="fa fa-check"></i><b>11.1</b> No flexibility: df = 0</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="comparing-models.html"><a href="comparing-models.html"><i class="fa fa-check"></i><b>12</b> Comparing models</a></li>
<li class="chapter" data-level="13" data-path="outside-of-the-normal.html"><a href="outside-of-the-normal.html"><i class="fa fa-check"></i><b>13</b> Outside of the normal</a></li>
<li class="chapter" data-level="14" data-path="remember-inference-isnt-everything.html"><a href="remember-inference-isnt-everything.html"><i class="fa fa-check"></i><b>14</b> Remember, inference isn’t everything</a></li>
<li class="divider"></li>
<li><a href="https://github.com/dtkaplan/Compact_inference" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Compact Guide to Classical Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="f-and-r" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> F and R</h1>
<p>We now have the pieces we need to assemble the central quantity which informs statistical inference. These are:<span class="math inline">\(\newcommand{\flex}[]{^\circ\!\cal{F}}\)</span></p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(n\)</span>, the sample size (or, more concretely, the number of rows in out data frame)</li>
<li><span class="math inline">\(v_r\)</span>, the variance of the response variable. <span class="math inline">\(v_r\)</span> for binary categorical response variables is based on the 0-1 encoding.</li>
<li><span class="math inline">\(v_m\)</span>, the variance of the model values.</li>
<li><span class="math inline">\(\flex\)</span>, the <em>degree of flexibility</em>.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></li>
</ol>
<p>We’ll put these together to form a quantity called F.</p>
<div id="the-f-statistic" class="section level2">
<h2><span class="header-section-number">8.1</span> The F statistic</h2>
<p>The name, F, is in honor of Ronald Fisher, one of the leading statisticians of the first half of the 20th. The formula for F is pretty simple, so I’ll present it right here for ready reference.</p>
<p><span class="math display">\[F \equiv \frac{n - (1 + \flex)}{\flex} \frac{v_m}{v_r - v_m}\]</span>
For almost all the settings considered in introductory statistics courses, <span class="math inline">\(\flex\)</span> is 1, so the formula simplifies to:
<span class="math display">\[F =  (n-1) \frac{v_m}{v_r - v_m}\]</span></p>
</div>
<div id="whats-the-meaning-of-f" class="section level2">
<h2><span class="header-section-number">8.2</span> What’s the meaning of F?</h2>
<p>F combines the four quantities <span class="math inline">\(n\)</span>, <span class="math inline">\(v_r\)</span>, <span class="math inline">\(v_m\)</span>, and <span class="math inline">\(\flex\)</span>. To get a notion why the combination works, keep these basic ideas in mind concerning what it means to have “more evidence.”</p>
<ul>
<li>The larger <span class="math inline">\(n\)</span>, the more evidence. That’s why F is more-or-less proportional to <span class="math inline">\(n\)</span>. (Strictly speaking, F is proportional to <span class="math inline">\(n - (\flex + 1)\)</span>.)</li>
<li>The more complicated the model – e.g. the number of explanatory variables or levels in an explanatory categorical variable – the less evidence. Or, put another way, we would want more evidence from data to justify a complicated model than a simple model. The division by <span class="math inline">\(\flex\)</span> in the formula for F implements this idea.</li>
<li>The closer the model values come to capturing the actual response variable, the greater the evidence that there is a relationship. An obvious
way to quantify this closeness are with the difference <span class="math inline">\(v_r - v_m\)</span>. We want the size of F to increase as <span class="math inline">\(v_m\)</span> gets closer to <span class="math inline">\(v_r\)</span>. So F is proportional to <span class="math inline">\(\frac{1}{v_r - v_m}\)</span>.</li>
<li>But the numerical value of the difference <span class="math inline">\(v_r - v_m\)</span> depends on the units in which the response variable is measured. For instance, we could express the running times in Chapter 1 in minutes or in seconds. But the difference <span class="math inline">\(v_r - v_m\)</span> would be <span class="math inline">\(60^2 = 3600\)</span> times larger if we used seconds than minutes. Obviously we don’t want our F value to depend on the units used. To avoid that, we divide <span class="math inline">\(v_r - v_m\)</span> by <span class="math inline">\(v_m\)</span>, getting the <span class="math inline">\(v_m / (v_r - v_m)\)</span> in the formula for F.</li>
</ul>
</div>
<div id="r-squared" class="section level2">
<h2><span class="header-section-number">8.3</span> R-squared</h2>
<p>Many people prefer to look at a ratio <span class="math inline">\(v_m / v_r\)</span> to quantify how close the model values are to the values of the response variable. If the model does a good job accounting for the response variable, then <span class="math inline">\(v_m\)</span> will be close to <span class="math inline">\(v_r\)</span>. That is, the ratio will be close to 1. On the other hand, if the model tells us little or nothing about the response variable, <span class="math inline">\(v_m\)</span> will be close to zero and the ratio itself will be zero.</p>
<p>The ratio has a famous name: <em>R-squared</em>, that is:</p>
<p><span class="math display">\[R^2 = v_m / v_r\]</span>
A more obscure name for <span class="math inline">\(R^2\)</span> is <em>coefficient of determination</em>, which is awkward but does express the point that <span class="math inline">\(R^2\)</span> is about the extent to which the explanatory variables, when passed through the model, determine the response variable. <span class="math inline">\(R^2\)</span> is, literally, the faction of the variance of the response variable that has been captured by the model.</p>
<p><span class="math inline">\(R^2\)</span> can never be bigger than one and can never be negative. When <span class="math inline">\(R^2 = 1\)</span>, the model values are exactly the same as the values of the response variable.</p>
<p>When there is no connection between the r esponse and explanatory variables, <span class="math inline">\(R^2\)</span> will be small. Ideally, it would be zero, but the process of random sampling generally pushes it a little away from zero. One way to think about F is as indicating when there is so little data that a small but non-zero R<sup>2</sup> is consistent with the hypothesis that there is no connection between the response and explanatory variables.</p>
</div>
<div id="f-in-statistics-books" class="section level2">
<h2><span class="header-section-number">8.4</span> F in statistics books</h2>
<p>In most statistics book, F is not written in the form above but in one of a couple of alternative – but equivalent – forms. There’s no particular reason to use these forms. Knowing what they look like will help you make sense of traditional statistical reports.</p>
<p>Since <span class="math inline">\(R^2\)</span> summarizes the relationship between <span class="math inline">\(v_m\)</span> and <span class="math inline">\(v_r\)</span>, the formula for F can be written in terms of <span class="math inline">\(R^2\)</span>. This is the first of the alternative forms.</p>
<p><span class="math display">\[F = \frac{n - (\flex+1)}{\flex} \frac{R^2}{1 - R^2}\]</span></p>
<p>Another alternative form comes from using an intermediate in the calculation of <span class="math inline">\(v_m\)</span> and <span class="math inline">\(v_r\)</span>. Recall how the variance is calculated by calculating square differences and averaging. To average, of course, you add together the quantities and then divide by the number of quantities being averaged.</p>
<p>Suppose you didn’t bother to average, and stopped after adding up the square differences. The name for this intermediate is the <em>sum of squares</em>.
F is often written in terms of the sum of squares of the response variable SS_r_ and of the model values SS_m_. Something like this:</p>
<p><span class="math display">\[F = \frac{n - (\flex+1)}{\flex} \frac{\mbox{SS}_m}{\mbox{SS}_r - \mbox{SS}_m}\]</span></p>
<p>More typically, instead instead of looking at the model values directly, the tradition in classical inference is to consider what’s called the <em>sum of squares of the residuals</em>, which is simply SSR = <span class="math inline">\(\mbox{SS}_r - \mbox{SS}_m\)</span> and the formula is re-written like this:</p>
<p><span class="math display">\[F = \frac{\mbox{SS}_m / \flex}{SSR / (n -  (\flex + 1))}.\]</span>
Both the numerator and the denominator of this ratio have the form of a sum of squares divided by a count. In the terminology of classical inference, such things are called <em>mean squares</em>.</p>
<p>In this book, we’ll just use the formula for F given at the start of this chapter. The others give exactly the same value, but let’s avoid having ton work with potentially confusing vocabulary such as the mean square and sum of squares.</p>
</div>
<div id="explaining-the-form-of-f" class="section level2">
<h2><span class="header-section-number">8.5</span> Explaining the form of F</h2>
<p>R<sup>2</sup> is an effective way of measuring how “close” the model values are to the response values. When R<sup>2</sup> <span class="math inline">\(= 1\)</span>, the model values are exactly equal to the values of the response variable.</p>
<p>Now recall that increasing the degrees of flexibility <span class="math inline">\(\flex\)</span> of a model–say be incorporating additional explanatory variables–enables the model values to come closer to the values of the response variable. With the vocabulary of R<sup>2</sup>, we can express this as “increasing <span class="math inline">\(\flex\)</span> increases R<sup>2</sup>.”</p>
<p>Common sense suggests that the explanatory variables used in a model should be somehow connected to the response variable; why else would you use them? But, in fact, the statement “increasing <span class="math inline">\(\flex\)</span> increases R<sup>2</sup>” is true even if the increased flexibility comes from including completely bogus explanatory variables in the model. As <span class="math inline">\(\flex \rightarrow n - 1\)</span>, regardless of the merit of the explanatory variables in the model, R<sup>2</sup> <span class="math inline">\(\rightarrow 1\)</span>.</p>
<p>So how to distinguish between a genuinely meaningful explanatory variable and a bogus, meaningless one? When you add either one to a model, R<sup>2</sup> will increase. What we ask when deciding whether an explanatory variable is meaningful or meaningless is <em>how much</em> did R<sup>2</sup> go up when the variable was added to the model. The sign that an explanatory variable is meaningful is that R<sup>2</sup> goes up by a lot, more than for a typical meaningless variable.</p>
<p>Let’s do a numerical experiment about R<sup>2</sup> and random, meaningless variables. Suppose we have a data frame with <span class="math inline">\(n=50\)</span>. Each of the variables will be constructed by a random-number generator, that is, they are completely unconnected one from the other. Call one of the variables <span class="math inline">\(y\)</span> and the others <span class="math inline">\(x_1, x_2, x_3, ...\)</span>, going up to, say, <span class="math inline">\(x_100\)</span>.</p>
<p>Setting the response variable to be y, we are going to fit a series of models with these explanatory variables. The first model will have just one explanatory variable: <span class="math inline">\(x_1\)</span>. This model has <span class="math inline">\(\flex = 1\)</span>. Fitting the model, calculate R<sup>2</sup>. We’ll want to keep track of this, so let’s label this <span class="math inline">\(\mbox{R}^2_1\)</span>. The second model will have two explanatory variables: <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> and therefore has <span class="math inline">\(\flex = 2\)</span>. Again, calculate R<sup>2</sup>. We’ll call this one <span class="math inline">\(\mbox{R}^2_2\)</span>. From the nature of model fitting, we know that <span class="math inline">\(\mbox{R}^2_1 \leq \mbox{R}^2_2\)</span>. Now fit a model with <span class="math inline">\(\flex = 3\)</span> using as explanatory variables <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, and <span class="math inline">\(x_3\)</span>. The R<sup>2</sup> for this model will be called <span class="math inline">\(\mbox{R}^2_3\)</span>. And so on, increasing <span class="math inline">\(\flex\)</span> one step at a time. In the end, we’ll have a sequence of R<sup>2</sup> for the collection of models which must show a non-decreasing pattern:</p>
<p><span class="math display">\[\mbox{R}^2_1 \leq \mbox{R}^2_2 \leq \mbox{R}^2_3 \leq \cdots \leq \mbox{R}^2_{100}\]</span></p>
<p>Figure 6.13 plots out this sequence of <span class="math inline">\(\mbox{R}^2_i\)</span> against <span class="math inline">\(\flex_i\)</span>. The sequence is shown as a dark black line. Note the steady (but somewhat random) increase until R<sup>2</sup> reaches 1 when <span class="math inline">\(\flex = n-1\)</span>. Figure 6.13 also shows several other similar trials, each of which follows it’s own random path up toward R<sup>2</sup> = 1 when <span class="math inline">\(\flex = n-1\)</span>. This is the signature of meaningless, bogus explanatory variables.</p>

<div class="figure"><span id="fig:R-path"></span>
<img src="images/14-R2-vs-m.png" alt="Figure 6.13: The dark path shows one trial in which \(\flex\) is steadily increased by using additional random explanatory variables. The other paths are for similar trials using new random data." width="250" />
<p class="caption">
Figure 8.1: Figure 6.13: The dark path shows one trial in which <span class="math inline">\(\flex\)</span> is steadily increased by using additional random explanatory variables. The other paths are for similar trials using new random data.
</p>
</div>
<p>IN DRAFT: Relabel x-axis as <span class="math inline">\(\flex\)</span>.</p>
<p>In actual practice, we’re not interested in using random explanatory variables. In fact, we’d prefer to avoid such bogus variables in favor of ones that are genuinely connected to the response variable. The picture we expect to see is shown in Figure 6.14.</p>

<div class="figure"><span id="fig:R-path2"></span>
<img src="images/14-model-walk-example.png" alt="Figure 6.14: \(\mbox{R}^2_i\) versus \(\flex\). The blue dot shows $^2_{i=10} for a model with \(\flex = 10\) where the explanatory variables have genuine explanatory power. Note that the path upward to the blue dot is much steeper than for the trials with random explanatory variables. Correspondingly, the path upward from the blue dot is less steep than for the trials with random explanatory variables." width="250" />
<p class="caption">
Figure 8.2: Figure 6.14: <span class="math inline">\(\mbox{R}^2_i\)</span> versus <span class="math inline">\(\flex\)</span>. The blue dot shows $^2_{i=10} for a model with <span class="math inline">\(\flex = 10\)</span> where the explanatory variables have genuine explanatory power. Note that the path upward to the blue dot is much steeper than for the trials with random explanatory variables. Correspondingly, the path upward <em>from</em> the blue dot is less steep than for the trials with random explanatory variables.
</p>
</div>
<p>The F statistic quantifies how much steeper is the path to the blue dot than the paths of the trials with random variables. Specifically, F is defined as a ratio of slopes: the slope upward to the blue dot divided by the slope upward <em>from</em> the blue dot.</p>
<p>We can calculate these two slopes since we know <span class="math inline">\(\flex\)</span>, R<sup>2</sup>, and <span class="math inline">\(n\)</span>.</p>
<ul>
<li>The slope <em>to</em> the blue dot is R<sup>2</sup><span class="math inline">\(/\flex\)</span>.</li>
<li>The slope <em>from</em> the blue dot is (1-R<sup>2</sup>)<span class="math inline">\(/(n - (\flex + 1))\)</span>.</li>
</ul>
<p>Taking the ratio, we have</p>
<p><span class="math display">\[F \equiv \frac{n - (\flex + 1)}{\flex} \frac{R^2}{1-R^2}\]</span>.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>If you are reading this book in conjunction with a conventional text, remember that such texts frame inference in terms of the <em>degrees of <strong>freedom</strong>,</em> df. The relationship is <span class="math inline">\(\mbox{df} \equiv n - (\flex + 1)\)</span>.<a href="f-and-r.html#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="effect-size.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="confidence-intervals.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["Compact_inference.pdf", "Compact_inference.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
