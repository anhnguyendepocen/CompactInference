[
["index.html", "A Compact Guide to Classical Inference Preface", " A Compact Guide to Classical Inference Daniel Kaplan 2019-11-27 Preface Statistical inference is the heart of many contemporary college-level statistics courses. For an introductory course, inference is generally taught in one of two ways. By far the most common uses algebraic formulas for standard errors and test statistics. Probability tables are then used to turn standard errors into confidence intervals and turn test statistics into p-values. Generations of students have been taught using the standard-error curriculum. Its difficulties and pedagogical shortcomings are well known, the common experience of every instructor using the curriculum. In response, an important alternative approach to inference has been developed based on simulation, randomization, and statistical bootstrapping. (See, e.g., Lock et al. (2017), Diez, Barr, and Çetinkaya-Rundel (2014), Tintle et al. (2016), Ismay and Kim (2019).) For many instructors, particularly those strongly oriented toward mathematics rather than computing, the formula-based line of attack seems natural. With formulas, there is a unique correct answer, while with randomization there is a process generating answers that differ (somewhat) from one realization to another. The formulas build on the strong mathematics tradition of using algebraic notation; they look like math. Computers are not needed; a calculator and a set of printed probability tables are sufficient to the task. The point about computers is particularly salient, since many instructors work in environments with insufficient support for computing infrastructure and lack experience with a computer-oriented pedagogy. Still, students in general have a hard time with algebra. Even for those select students who are confident reading and interpreting formulas, the tractable formulas for inference work only with simple statistics – means, proportions, slopes – in settings with at most two variables. When it comes time to deal with other statistical settings, new and seemingly unrelated methods are introduced. For instance, inference on tables of counts or on multiple means does not involve calculating a standard error, but uses other statistical procedures such as \\(\\chi^2\\) and ANOVA, each of which comes with a new table for the corresponding probability distributions. For multiple regression, inferential formulas are not readily available to students. So students in introductory statistics do not have access to powerful methods that are mainstream in statistical investigations, that take on the challenge of confounding, and that help to reveal the relationships among multiple variables. Both students and instructors perceive standard-error statistics as a confusing collection of specialized tools. To improve student learning, instructors long for a reduction in the number of topics needed to support statistical thinking. This book is a roadmap for instructors who wish to streamline inference while continuing to teach using traditional tools. Simplified does not mean simplistic. The strategy for teaching provided by this book produces answers that fully comply with legitimate uses of statistical inference. How? Conventionally, the logic of introductory inference recapitulates the historical route to the development of statistical concepts from 1880 to 1910. Instead of following every twist and turn in that path, this book uses modern modeling terminology – response and explanatory variables, functions, model output – and the concepts of analysis of variance developed around 1925. I believe that randomization is an excellent conceptual framework for an introduction to statistics and that exposure to computational tools should be an important component of every student’s education. But for those who are not ready to embrace computation, or not in a situation where they can do so, the strategy introduced in this book can, I hope, improve student understanding. Beyond that, the simplifications can provide curricular space for the methods of statistics and data science needed to make sense of modern data in contemporary settings. Daniel Kaplan, December 2019, Saint Paul, Minnesota References "]
]
